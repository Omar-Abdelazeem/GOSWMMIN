{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a SWMMIN Simulation\n",
    "This notebook takes an input EPANET file with demands input normally as a CWS base demand and outputs a .inp file configured for SWMM in the recommended SWMMIN Method  \n",
    "A simplified schematic of the modified demand node in this method is seen below:  \n",
    "\n",
    "![Figure 1](../Figures/Figure%20Files/Figure%201.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we import the necessary libraries and packages\n",
    "**WNTR** for building EPANET network models in Python  \n",
    "**NUMPY & PANDAS** for data handling and processing  \n",
    "**re** for searching and matching text in the .inp file using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wntr             # For EPANET file reading\n",
    "import numpy as np      \n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import pathlib "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying paths for EPANET.inp File to be Converted and preprocessing the input\n",
    "**Warning:** *Paths in this script (and the rest of this repo) are absolute unless running the network files provided within the repo*  \n",
    "Input filename (with extensions) as string.  \n",
    "For running the .inp files in this repository, you can use this relative path `\"../Network-Files/Network X/\"` where X is the network number  \n",
    "Will work with both Windows and POSIX paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File:  Linear_Network\n"
     ]
    }
   ],
   "source": [
    " # Replace with appropriate path and filename\n",
    "directory=pathlib.Path(\"../Networks/Linear Network\")\n",
    "filename=pathlib.Path(\"Linear_Network.inp\")\n",
    "name_only=filename.stem\n",
    "print(\"Selected File: \",name_only)\n",
    "abs_path=directory/filename"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Assumptions \\& Inputs\n",
    "\n",
    "### Pressure Dependent Withdrawal\n",
    "For the Withdrawal Outlet, the PDW formulation adopted by SWMMIN is a modified version of Wagner et al. (1988)'s formula:\n",
    "\n",
    "$$ Q_{PDW}\\, = \\!Q_{des}(\\frac{\\hat H-H^{min}}{H^{des}-H^{min}})^{1/n} \\quad[1]$$ \n",
    "Where $Q_{PDW}$ is the flow through the service connection, $Q_{des}$ is the desired (base) demand, $\\hat H$ is either the pressure at the demand node or the pressure difference across the withdrawal outlet $j$, $H^{min}$ is the minimum head, and $H^{des}$ is the desired head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Pressure Dependent Withdrawal (PDW)\n",
    "desired_pressure= 10        # Set the desired pressure\n",
    "minimum_pressure= 0         # Set the minimum pressure\n",
    "pdw_exponent = 0.5          # Set the exponent for the pressure dependent withdrawal\n",
    "pdw_variable = \"PRESSURE\"   # Whether the pressure difference across the outlet (PRESSURE) or the freeboard depth at the node (DEPTH) is used for the PDW\n",
    "desired_flow = None         # path to CSV for desired flow rates (if None, the flow rate is calculated based on the demand and the supply duration)\n",
    "pressure_diff=desired_pressure-minimum_pressure  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Discretization\n",
    "\n",
    "Modelers can control the spatial resolution of the discretization in three ways:  \n",
    "    1- Adaptive discretization based on a Length to Diameter Ratio ($L/D$). Pipes will be broken down into segments of length $L/D \\times D$ based on each pipe's diameter.  \n",
    "    2- Uniform discretization based on a Length to Diameter Ratio. Pipes will be broken down into segments based on the largest pipe's diameter ($\\Delta x_{max} = L/D \\times D_{max}$).  \n",
    "    3- Uniform discretization based on a flat maximum length. Pipes will be broken down into segments equal to or less than the assigned $\\Delta x_{max}$.  \n",
    "\n",
    "Assigning a ```maximum_xdelta``` other than 0 overrides the first two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Spatial Discretization\n",
    "len_to_diameter_ratio = 30  # decrease for finer resolutions (more pipes), increase for coarser resolutions (less pipes)\n",
    "adaptive = False            # Set to True for adaptive discretization, False for non-adaptive discretization\n",
    "maximum_xdelta = 0          # Overrides previous settings, applies a flat discretization to all pipes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Discretization  \n",
    "\n",
    "The timestep can be set using a recommended solution speed $v_{sol}$ between 50-200 m/s; the default is 100.  \n",
    "Alternatively, a specified timestep can override the solution-speed based timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Discretization\n",
    "solution_speed = 100        # recommended range 50-200 m/s\n",
    "timestep = None             # Set to None for automatic timestep calculation, otherwise set the timestep in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 1                  # Number of Simulation Days\n",
    "supply_duration_inp = 8.0   # Number of hours the supply is available in decimals, i.e., input 3.5 hours for 3:30 (if different from .inp file duration)\n",
    "\n",
    "# Storage Volumes\n",
    "tank_height = 1            # Height of the tank in meters; path (string) to csv file for non-uniform tank heights\n",
    "tank_areas  = None         # Areas of the tanks in square meters computed from demands by default; path (string) to csv file for non-uniform tank areas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information from the EPANET file\n",
    "To modify the .inp file, demand junction IDs, elevations, x and y coordinates  \n",
    "We use wntr to build the network model of the input file and use wntr's junctions module to extract the details of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_nodes=[]       # For storing list of nodes that have non-zero demands\n",
    "desired_demands=[]    # For storing demand rates desired by each node for desired volume calculations\n",
    "elevations=[]         # For storing elevations of demand nodes\n",
    "coords=dict()         # For storing coordinates corresponding to each node as a tuple with the id as key\n",
    "all_nodes=[]          # For storing list of node ids of all nodes\n",
    "all_elevations=[]     # For storing elevations of all nodes\n",
    "\n",
    "# Creates a network model object using EPANET .inp file\n",
    "network=wntr.network.WaterNetworkModel(abs_path)\n",
    "\n",
    "# Iterates over the junction list in the Network object\n",
    "for node in network.junctions():\n",
    "    all_nodes.append(node[1].name)\n",
    "    all_elevations.append(node[1].elevation)\n",
    "    coords[node[1].name]=node[1].coordinates\n",
    "    # For all nodes that have non-zero demands\n",
    "    if node[1].base_demand != 0:\n",
    "        # Record node ID (name), desired demand (base_demand) in CMS, elevations, x and y coordinates\n",
    "        demand_nodes.append(node[1].name)\n",
    "        desired_demands.append(node[1].base_demand)\n",
    "        elevations.append(node[1].elevation)\n",
    "        \n",
    "\n",
    "conduit_ids= []       # To store IDs of the original pipes in the EPANET file\n",
    "conduit_from= []      # To store the origin node for each pipe\n",
    "conduit_to= []        # To store the destination node for each pipe\n",
    "conduit_lengths= []   # To store pipe lengths\n",
    "conduit_diameters= [] # To store pipe diameters\n",
    "\n",
    "# Loop over each link in the EPANET model\n",
    "for link in network.pipes():\n",
    "\n",
    "    # Extract and store each of the aforementioned properties\n",
    "    conduit_ids.append(link[1].name)\n",
    "    conduit_from.append(link[1].start_node_name)\n",
    "    conduit_to.append(link[1].end_node_name)\n",
    "    conduit_lengths.append(link[1].length)\n",
    "    conduit_diameters.append(link[1].diameter)\n",
    "\n",
    "pump_ids= []       # To store IDs of the original pumps in the EPANET file\n",
    "pump_from= []      # To store the origin node for each pump\n",
    "pump_to= []        # To store the destination node for each pump\n",
    "pump_curves = []   # To store the name of the pump's curve\n",
    "\n",
    "\n",
    "# Loop over each pump in the EPANET model\n",
    "for pump in network.pumps():\n",
    "\n",
    "    # Extract and store each of the aforementioned properties\n",
    "    pump_ids.append(pump[1].name)\n",
    "    pump_from.append(pump[1].start_node_name)\n",
    "    pump_to.append(pump[1].end_node_name)\n",
    "    pump_curves.append(pump[1]._pump_curve_name)\n",
    "\n",
    "\n",
    "reservoir_ids=[]      # To store the source reservoirs' IDs\n",
    "reservoir_heads={}    # To store the total head of each reservoir indexed by ID\n",
    "reservoir_coords={}   # To store the coordinates as tuple (x,y) indexed by ID\n",
    "\n",
    "# Loops over each reservoir\n",
    "for reservoir in network.reservoirs():\n",
    "    reservoir_ids.append(reservoir[1].name)\n",
    "    reservoir_heads[reservoir_ids[-1]]=reservoir[1].base_head\n",
    "    reservoir_coords[reservoir_ids[-1]]=reservoir[1].coordinates\n",
    "reservoir_elevations={reservoir:reservoir_heads[reservoir]-30 for reservoir in reservoir_heads}\n",
    "\n",
    "pcurves = {}\n",
    "for curve in network.curves():\n",
    "    pcurves[curve[0]]= curve[1].points\n",
    "\n",
    "# Get the supply duration in minutes (/60) as an integer\n",
    "supply_duration=int(network.options.time.duration/60)\n",
    "if supply_duration_inp:\n",
    "    supply_duration=int(supply_duration_inp*60)\n",
    "supply_hh=str(supply_duration//60)     # The hour value of the supply duration (quotient of total supply in minutes/ 60)\n",
    "supply_mm=str(supply_duration%60)      # The minute value of the supply duration (remainder)\n",
    "\n",
    "# Corrects the formatting of the HH:MM by adding a 0 if it is a single digit: if minutes =4 -> 04\n",
    "if len(supply_mm)<2:\n",
    "    supply_mm='0'+supply_mm\n",
    "if len(supply_hh)<2:\n",
    "    supply_hh='0'+supply_hh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Pipes\n",
    "Adjusts the conduit information such that pipes are matched concentrically rather than SWMM's default of invert to invert matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe aggregating all node information gathered from the EPANET file\n",
    "junctions=pd.DataFrame(zip(all_nodes,all_elevations,coords.values()),columns=[\"ID\",\"Elevation\",\"Coordinates\"])\n",
    "# Set the junction ID as the index of the Dataframe\n",
    "junctions.set_index(\"ID\",inplace=True)\n",
    "\n",
    "# Dataframe aggregating all conduit information gathered from the EPANET file\n",
    "conduits=pd.DataFrame(zip(conduit_ids,conduit_from,conduit_to,conduit_lengths,conduit_diameters),columns=[\"ID\",\"from node\",\"to node\",\"Length\",\"diameter\"])\n",
    "# Set the conduit ID as the index\n",
    "conduits.set_index(\"ID\",inplace=True)\n",
    "\n",
    "concentric=True #If this is False, Pipes will be matched using inverts\n",
    "\n",
    "connectivity=pd.DataFrame(index=junctions.index, columns=[\"US\",\"DS\"])\n",
    "\n",
    "if concentric:\n",
    "    # For each node in the network\n",
    "    for junction in junctions.index:\n",
    "        upst_connected=[] #List of pipes whose upstream end connects to this node\n",
    "        downst_connected=[] #List of pipes whose downstream end connects to this node\n",
    "        # Loop over all conduits\n",
    "        for conduit in conduits.index:\n",
    "            # if conduit starts at this node\n",
    "            if conduits.at[conduit,\"from node\"]==junction:\n",
    "                # append to upstream connected list\n",
    "                upst_connected.append(conduit)\n",
    "            # if conduit ends at this node\n",
    "            elif conduits.at[conduit,\"to node\"]==junction:\n",
    "                # append to downstream connected list\n",
    "                downst_connected.append(conduit)\n",
    "        # create a list of all connected conduits\n",
    "        connected_conduits=upst_connected+downst_connected\n",
    "        # Get the diameter of all connected conduits\n",
    "        connected_diameters=[conduits.at[conduit,\"diameter\"] for conduit in connected_conduits]\n",
    "        # Find the diameter of the biggest pipe connected to this node\n",
    "        max_diameter=max(connected_diameters)\n",
    "\n",
    "        for conduit in upst_connected:\n",
    "            # If pipe is not the biggest pipe\n",
    "            if conduits.at[conduit,\"diameter\"]<max_diameter:\n",
    "                # offset this pipe by half the difference in their diameters\n",
    "                conduits.at[conduit,\"InOffset\"]=(max_diameter-conduits.at[conduit,\"diameter\"])/2\n",
    "            # if it is the biggest pipe do nothing\n",
    "            else: conduits.at[conduit,\"InOffset\"]=0\n",
    "\n",
    "        for conduit in downst_connected:\n",
    "            # if not the biggest connected pipe to this node\n",
    "            if conduits.at[conduit,\"diameter\"]<max_diameter:\n",
    "                # offset this pipe by half the difference in their diameters\n",
    "                conduits.at[conduit,\"OutOffset\"]=(max_diameter-conduits.at[conduit,\"diameter\"])/2\n",
    "            else: conduits.at[conduit,\"OutOffset\"]=0\n",
    "    \n",
    "        connectivity.at[junction,\"US\"]=upst_connected\n",
    "        connectivity.at[junction,\"DS\"]=downst_connected\n",
    "        connectivity.at[junction,\"Max D\"]=max_diameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Discretization of Longer Pipes\n",
    "\n",
    "The following cell breaks down pipes that are longer than a specified maximum $\\Delta x_{max}$  \n",
    "Pipes longer than the maximum length are divided into equal parts, where the number of parts is:   \n",
    "$$ N_{parts}= \\left\\lceil \\frac{L_{pipe}}{\\Delta x_{max}} \\right\\rceil$$\n",
    "The length of each part is then set at:\n",
    "$$L_{part}=\\frac{L_{pipe}}{N_{parts}}$$  \n",
    "\n",
    "The $\\Delta x_{max}$ is determined using the length to diameter ratio and a) each pipe's diameter if adaptive discretization is chosen or b) the largest pipe diameter in the network if not adaptive. Alternatively, it could be input directly and overrides the other two options  \n",
    "\n",
    "After creating all pipe segments, intermediate nodes are created to join the pipe segments.  \n",
    "The elevation of these nodes (as well as the x and y coordinates) are interpolated linearly using the start elevation and the end elevation,  \n",
    " where the difference in elevation between each two consecutive nodes set as:\n",
    "$$\\Delta z = \\frac{ z_{end}-z_{start}}{N_{parts}}$$\n",
    "Similarly, the difference in x and y coordinates are found as:  \n",
    "$$\\Delta x = \\frac{ x_{end}-x_{start}}{N_{parts}}\\\\\n",
    "\\Delta y = \\frac{ y_{end}-y_{start}}{N_{parts}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_pipes=[]\n",
    "\n",
    "if maximum_xdelta==0:\n",
    "    maximum_xdelta=conduits[\"diameter\"].max() * len_to_diameter_ratio\n",
    "\n",
    "# Loop over each conduit in the original file\n",
    "for conduit in conduits.index:\n",
    "\n",
    "    length=conduits[\"Length\"][conduit]  #Stores the length of the current conduit for shorthand\n",
    "    diameter = conduits[\"diameter\"][conduit]\n",
    "    if adaptive:\n",
    "        n_parts = np.max(round(length / diameter / len_to_diameter_ratio) - 1, 0)\n",
    "    else: n_parts = math.ceil(length / maximum_xdelta)\n",
    "\n",
    "    xdelta = length / n_parts\n",
    "\n",
    "    # If the conduit is bigger than the maximum allowable length (delta x), we will break it down into smaller pipes\n",
    "    if n_parts>0:\n",
    "        # Calculate the length of each part \n",
    "        part_length=length/n_parts\n",
    "        # Start node ID (for shorthand)\n",
    "        start_node=conduits[\"from node\"][conduit]\n",
    "        # Inlet Offset for the Current Conduit\n",
    "        in_offset=conduits[\"InOffset\"][conduit]\n",
    "        # End node ID (for shorthand)\n",
    "        end_node=conduits[\"to node\"][conduit]\n",
    "        #Outlet Offset for the Current Conduit\n",
    "        out_offset=conduits[\"OutOffset\"][conduit]\n",
    "        # If the start node is a reservoir\n",
    "        if start_node in reservoir_ids:\n",
    "            # MAke the start elevation the same as the end but add 1 (since reservoirs don't have ground elevation in EPANET)\n",
    "            start_elevation=junctions.at[end_node,\"Elevation\"]+1\n",
    "            reservoir_elevations[start_node]=start_elevation+1\n",
    "        # Otherwise make the start elevation equal to the elevation of the start node + the offset of the pipe\n",
    "        else: start_elevation=junctions.at[start_node,\"Elevation\"]+in_offset\n",
    "        \n",
    "        # If the end node is a reservoir\n",
    "        if end_node in reservoir_ids:\n",
    "            # Make the end elevation the same as the start but subtract 1 (since reservoirs don't have ground elevation in EPANET)\n",
    "            end_elevation=start_elevation-1\n",
    "        # Make the end elevation equal to the elevation of the end node + the offset of the pipe\n",
    "        else: end_elevation=junctions.at[end_node,\"Elevation\"]+out_offset\n",
    "        # Calculate the uniform drop (or rise) in elevation for all the intermediate nodes about to be created when this pipe is broken into several smaller ones\n",
    "        unit_elev_diff=(end_elevation-start_elevation)/n_parts\n",
    "\n",
    "        # if the starting node is a reservoir\n",
    "        if start_node in reservoir_ids:\n",
    "            # Get coordinates from reservoir data\n",
    "            start_x=reservoir_coords[start_node][0]\n",
    "            start_y=reservoir_coords[start_node][1]\n",
    "        else:\n",
    "            # Get the coordinates from the junction data\n",
    "            start_x=junctions.at[start_node,\"Coordinates\"][0]\n",
    "            start_y=junctions.at[start_node,\"Coordinates\"][1]\n",
    "        \n",
    "        # If the end node is a reservoir\n",
    "        if end_node in reservoir_ids:\n",
    "            # Get the coordinates from the reservoir data\n",
    "            end_x=reservoir_coords[end_node][0]\n",
    "            end_y=reservoir_coords[end_node][1]\n",
    "        else:\n",
    "            # Get them from the junctions data\n",
    "            end_x=junctions.at[end_node,\"Coordinates\"][0]\n",
    "            end_y=junctions.at[end_node,\"Coordinates\"][1]\n",
    "            \n",
    "        # Calculate the unit difference in x and y coordinates for this pipe and its segments\n",
    "        unit_x_diff=(end_x-start_x)/n_parts\n",
    "        unit_y_diff=(end_y-start_y)/n_parts\n",
    "\n",
    "\n",
    "# THIS LOOP GENERATES THE SMALLER PIPES TO REPLACE THE ORIGINAL LONG PIPE\n",
    "        # For each part to be created\n",
    "        for part in np.arange(1,n_parts+1):\n",
    "\n",
    "            # CREATING THE LINKS\n",
    "            # Create the ID for the new smaller pipe as OriginPipeID-PartNumber\n",
    "            new_id=conduit+\"-\"+str(part)\n",
    "            # Set the new pipe's diameter equal to the original one\n",
    "            conduits.at[new_id,\"diameter\"]=conduits[\"diameter\"][conduit]\n",
    "            # Set the start node as OriginStartNode-NewNodeNumber-OriginEndNode  as in the first intermediate nodes between node 13 and 14 will be named 13-1-14\n",
    "            conduits.at[new_id,\"from node\"]=start_node+\"-\"+str(part-1)+\"-\"+end_node\n",
    "\n",
    "            # Set Inlet and Outlet Offsets to Zero by default\n",
    "            conduits.at[new_id,\"InOffset\"]=0\n",
    "            conduits.at[new_id,\"OutOffset\"]=0\n",
    "\n",
    "            # if this is the first part, use the original start node \n",
    "            if part==1:\n",
    "                conduits.at[new_id,\"from node\"]=start_node\n",
    "                # Set the first inlet offset to concentrically match the upstream pipe\n",
    "                conduits.at[new_id,\"InOffset\"]=in_offset\n",
    "                \n",
    "            # Set the end node as OriginStartNode-NewNodeNumber+1-OriginEndNode  as in the second intermediate nodes between node 13 and 14 will be named 13-2-14\n",
    "            conduits.at[new_id,\"to node\"]=start_node+\"-\"+str(part)+\"-\"+end_node\n",
    "            # If this is the last part, use the original end node as the end node\n",
    "            if part==n_parts:\n",
    "                conduits.at[new_id,\"to node\"]=end_node\n",
    "                # Set the last outlet offset to concentrically match the downstream pipe\n",
    "                conduits.at[new_id,\"OutOffset\"]=out_offset\n",
    "            # Set the new pipe's length to the length of each part\n",
    "            conduits.at[new_id,\"Length\"]=part_length\n",
    "\n",
    "            # if this is NOT the last part (as the last pipe segment joins a pre-existing node and does not need a node to be created)\n",
    "            if part<n_parts:\n",
    "                # Create a new node at the end of this pipe segment whose elevation is translated from the start elevation using the unit slope and the part number\n",
    "                junctions.at[conduits.at[new_id,\"to node\"],\"Elevation\"]=start_elevation+part*unit_elev_diff\n",
    "                # Calculate the coordinates for the new node using the unit difference in x and y coordinates\n",
    "                junctions.at[conduits.at[new_id,\"to node\"],\"Coordinates\"]=(start_x+part*unit_x_diff,start_y+part*unit_y_diff)\n",
    "\n",
    "            if conduits.at[new_id,\"from node\"] in reservoir_ids or conduits.at[new_id,\"to node\"] in reservoir_ids:\n",
    "                reservoir_pipes.append(new_id)\n",
    "            \n",
    "        # After writing the new smaller pipes, delete the original pipe (since it is now redundant)\n",
    "        conduits.drop(conduit,inplace=True)\n",
    "\n",
    "conduits[[\"InOffset\",\"OutOffset\"]]=conduits[[\"InOffset\",\"OutOffset\"]].fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Junctions\n",
    "The junction section lines are written. Data required for the junctions include:\n",
    "- Name: Already stored in junctions  \n",
    "- Elevation: Already stored in junctions  \n",
    "- MaxDepth: 0  \n",
    "- Initial Depth (InitDepth): 0 No initialization required  \n",
    "- Surcharge Depth (SurDepth): 100 or any value high enough to prevent the node from overflowing (to simulate a pressurized pipe)  \n",
    "- Area Ponded (Aponded): 0  No ponding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxDepth=[0]*len(junctions)\n",
    "InitDepth=MaxDepth\n",
    "SurDepth=[100] * len(junctions)  # High value to prevent surcharging\n",
    "Aponded=InitDepth\n",
    "\n",
    "# Creates dataframe with each row representing one line from the junctions section\n",
    "junctions_section=pd.DataFrame(list(zip(junctions.index,junctions[\"Elevation\"],MaxDepth,InitDepth,SurDepth,Aponded)))\n",
    "# Converts the dataframe into a list of lines in the junctions section\n",
    "junctions_section=junctions_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "# adds a new line character to the end of each line in the section\n",
    "junctions_section=[line+'\\n' for line in junctions_section]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outfalls\n",
    "For each demand node, two outfalls are created to receive the consumption rate ($Q_{cons}$) and the leakage.  \n",
    "- Data required for outfalls:  \n",
    "- Name: formatted as OutfallX where X is the ID of the original demand node  \n",
    "- Elevation: equal to the original demand node's elevation  \n",
    "- Type: FREE no boundary conditions forced  \n",
    "- Stage Data: Blank None provided  \n",
    "- Gated: NO  \n",
    "- Route To: Blank None specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(demand_nodes)\n",
    "outfall_ids=[\"Outfall\"+ str(node) for node in demand_nodes]\n",
    "outfall_elevations=elevations\n",
    "outfall_types=['FREE' for i in demand_nodes]\n",
    "stage_data=[\"    \" for i in demand_nodes]\n",
    "outfall_gated=[\"NO\" for i in demand_nodes]\n",
    "# Creates dataframe with each row representing one line from the outfalls section\n",
    "outfall_section=pd.DataFrame(zip(outfall_ids,outfall_elevations,outfall_types,stage_data,outfall_gated))\n",
    "\n",
    "\n",
    "leak_outfall_id=[\"L_Outfall\"+str(node) for node in demand_nodes]\n",
    "\n",
    "leak_outfalls=pd.DataFrame(zip(leak_outfall_id,outfall_elevations,outfall_types,stage_data,outfall_gated))\n",
    "outfall_section=pd.concat([outfall_section,leak_outfalls])\n",
    "# Converts the dataframe into a list of lines in the outfalls section\n",
    "outfall_section=outfall_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "# adds a new line character to the end of each line in the section\n",
    "outfall_section=[line+'\\n' for line in outfall_section]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Storage Nodes\n",
    "Two types of storage nodes are needed in a SWMMIN\n",
    "First, storage nodes that represent the sources (reservoirs) present in the original file.  \n",
    "Second, each demand node is connected to a storage node whose volume is defined using a funcitonal input such that:\n",
    "$$ V_{tank} = A_{tank} \\times h_{tank}$$  \n",
    "The tank areas are inferred from the demand rates in the EPANET .inp file such that each demand node has one day's demand's worth of storage. Alternatively, modelers can input the tank areas manually in the form of a CSV file.   \n",
    "Tank heights can be set uniformly (default is 1 m) or by passing a CSV with each tank's height\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Storage IDS (correspond to original DN ID)\n",
    "storage_ids=[\"StorageforNode\"+id for id in demand_nodes]\n",
    "# Calculate the area of each storage based on their demand\n",
    "storage_areas=[demand*60* supply_duration/tank_height for demand in desired_demands]\n",
    "# demand_volumes = pd.DataFrame({'ID':demand_nodes,'Volume':storage_areas})\n",
    "# demand_volumes.to_csv(abs_path.parent/pathlib.Path(name_only+'_DemandVolumes.csv'))\n",
    "# OVERRIDE OPTION: Input Tank Areas Manually\n",
    "if tank_areas is not None:\n",
    "    storage_areas = pd.read_csv(tank_areas,header=None)\n",
    "    storage_areas.columns = ['Area']\n",
    "    storage_areas = storage_areas['Area'].to_list()\n",
    "    storage_areas = [area*60* supply_duration/tank_height for area in storage_areas]\n",
    "# Set storage invert elevations = elevation of DN + Hmin\n",
    "storage_elevations=[elevation+minimum_pressure for elevation in elevations]\n",
    "if type(tank_height) == str:\n",
    "    storage_MaxDepth = pd.read_csv(tank_height,header=None)\n",
    "    storage_MaxDepth.columns = ['Height']\n",
    "    storage_MaxDepth = storage_MaxDepth['Height'].to_list()\n",
    "else: storage_MaxDepth=[tank_height]*len(storage_ids)\n",
    "\n",
    "storage_InitDepth=[0]*len(storage_ids)\n",
    "storage_shape=[\"FUNCTIONAL\"]*len(storage_ids)\n",
    "zeroes=['0']*len(storage_ids)    # for the other curve parameters which are not required for circular tanks\n",
    "storage_SurDepth=[0]*len(storage_ids)\n",
    "storage_fevap=[0]*len(storage_ids)\n",
    "\n",
    "storage_units=pd.DataFrame(zip(storage_ids,storage_elevations,storage_MaxDepth,storage_InitDepth,storage_shape,zeroes,zeroes,storage_areas,storage_SurDepth,storage_fevap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage Nodes that Replace EPANET Reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename reservoirs\n",
    "reservoir_ids_new=[\"Reservoir-\"+str(id) for id in reservoir_ids]\n",
    "# update references to reservoirs in conduits\n",
    "for i in range(0,len(reservoir_ids)):\n",
    "    conduits.replace(to_replace=reservoir_ids[i],value=reservoir_ids_new[i])\n",
    "# set reservoir elevations\n",
    "reservoir_elevations=reservoir_elevations.values()\n",
    "# set max depth of reservoirs\n",
    "MaxDepth=[max(100,max(reservoir_heads.values())+10)]*len(reservoir_ids)\n",
    "# Set initial depth of reservoirs\n",
    "InitDepth=[head-elevation for head, elevation in zip(reservoir_heads.values(),reservoir_elevations)]\n",
    "reservoir_shape=[\"TABULAR\"]*len(reservoir_ids)\n",
    "reservoir_curves=[\"Source\"+id for id in reservoir_ids]\n",
    "blanks=['    ']*len(storage_ids)  \n",
    "reservoir_SurDepth=[0]*len(reservoir_ids)\n",
    "reservoir_psi=[0]*len(reservoir_ids)\n",
    "\n",
    "storage_section=pd.DataFrame(zip(reservoir_ids_new,reservoir_elevations,MaxDepth,InitDepth,reservoir_shape,reservoir_curves,blanks,blanks,reservoir_SurDepth,reservoir_psi))\n",
    "storage_section= pd.concat([storage_section,storage_units])\n",
    "storage_section=storage_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "storage_section=[line+'\\n' for line in storage_section]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Conduits\n",
    "Name From To L Roughness(manning) InOff  OutOff InitFlow  MaxFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conduit_ids=[\"P-\"+str(conduit) if str(conduit)[0]!=\"P\" else conduit for conduit in conduits.index]\n",
    "roughness=[0.011]*len(conduits)\n",
    "conduit_zeros=[0]*len(conduits)\n",
    "conduits[\"from node\"].replace(reservoir_ids,reservoir_ids_new,inplace=True)\n",
    "conduits[\"to node\"].replace(reservoir_ids,reservoir_ids_new,inplace=True)\n",
    "\n",
    "\n",
    "conduits_section=pd.DataFrame(zip(conduit_ids,conduits[\"from node\"],conduits[\"to node\"],conduits[\"Length\"],roughness,conduits[\"InOffset\"],conduits[\"OutOffset\"],conduit_zeros,conduit_zeros))\n",
    "conduits_section=conduits_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "conduits_section=[line+'\\n' for line in conduits_section]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outlets\n",
    "To have a rating curve with no upper limit, we use the FUNCTIONAL mode of definition in relation to the depth (Pressure) at the original demand node  \n",
    "Functional definition of rating curves is specified as:\n",
    "$$ Q_{outlet}=c d^{\\alpha}$$\n",
    "where d is teh depth, c is a coefficient and alpha is an exponent  \n",
    "We aim to approximate the head-flow relationship (above) using this definition, by using the same exponent 0.5 and calculating the coefficient such that:  \n",
    "$$ Q=Q_{des} \\text{ when } d=H^{des}-H^{min}$$\n",
    "Thus the function's coefficient can be found as:  \n",
    "$$ c=\\frac{Q_{des}}{\\sqrt{H^{des}-H^{min}}}$$\n",
    "Since the depth at the node is equal to $H_j$, this approximation is exact when $H_{min}=0$ \n",
    "$$ Q_{outlet}=Q_{des}\\sqrt{\\frac{H_j}{{H^{des}-H^{min}}}}$$\n",
    "Lastly, an outlet is needed to restrict the flow to the artificial outfall to a negligible amount by setting the coefficient to a sufficiently small value and the exponent to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Outlet\n",
    "outlet_ids = [\"Outlet\"+id for id in demand_nodes]\n",
    "outlet_from = demand_nodes[:]\n",
    "outlet_to = storage_ids [:]\n",
    "outlet_offset=[connectivity.at[node,\"Max D\"]/2 for node in demand_nodes]\n",
    "# Change to FUNCTIONAL/PRESSURE for head difference PDW, rather than upstream depth\n",
    "outlet_type=[\"FUNCTIONAL/\"+pdw_variable]*len(outlet_ids)\n",
    "# Desired demands are taken as base demands of EPANET input file by default. To change this, either update the EPANET input file or input a new set of desired demands to desired_demands\n",
    "outlet_coeff=[demand*1000/np.sqrt(pressure_diff) for demand in desired_demands]  \n",
    "# Change to change PDW exponent\n",
    "outlet_expon=[\"0.5\"]*len(outlet_ids)\n",
    "outlet_gated=[\"YES\"]*len(outlet_ids)\n",
    "\n",
    "# Demand Outlets\n",
    "outdemand_ids = [\"DemandOutlet\"+id for id in demand_nodes]\n",
    "outdemand_from = storage_ids [:]\n",
    "outdemand_to = outfall_ids [:]\n",
    "outdemand_offset=[0]*len(outlet_ids)\n",
    "outdemand_type=[\"TABULAR/DEPTH\"]*len(outlet_ids)\n",
    "outdemand_coeff=[\"Demand\"+ id for id in demand_nodes]\n",
    "outdemand_expon=[\"     \"]*len(outlet_ids)\n",
    "outdemand_gated=[\"YES\"]*len(outlet_ids)\n",
    "\n",
    "outlets=pd.DataFrame(list(zip(outlet_ids,outlet_from,outlet_to,outlet_offset,outlet_type,outlet_coeff,outlet_expon,outlet_gated)))\n",
    "outletdemand=pd.DataFrame(list(zip(outdemand_ids,outdemand_from,outdemand_to,outdemand_offset,outdemand_type,outdemand_coeff,outdemand_expon,outdemand_gated)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Leakage Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_fraction = 0.1\n",
    "\n",
    "leak_ids=[\"LeakforNode\"+str(node) for node in demand_nodes]\n",
    "leak_from=demand_nodes[:]\n",
    "leak_to=leak_outfall_id[:]\n",
    "leak_offset=[connectivity.at[node,\"Max D\"]/2 for node in demand_nodes]\n",
    "leak_type=[\"FUNCTIONAL/DEPTH\"]*len(leak_ids)\n",
    "leak_coeff=[demand*1000/np.sqrt(pressure_diff)*leak_fraction for demand in desired_demands]\n",
    "leak_expon=[\"0.5\"]*len(leak_ids)\n",
    "leak_gated=[\"YES\"]*len(leak_ids)\n",
    "\n",
    "leak_outlets=pd.DataFrame(zip(leak_ids,leak_from,leak_to,leak_offset,leak_type,leak_coeff,leak_expon,leak_gated))\n",
    "\n",
    "outlet_section=pd.concat([outlets,outletdemand,leak_outlets])\n",
    "outlet_section=outlet_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "outlet_section=[line+'\\n' for line in outlet_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Pumps\n",
    "[PUMPS]  \n",
    ";;Name           From Node        To Node          Pump Curve       Status   Sartup Shutoff   \n",
    ";;-------------- ---------------- ---------------- ---------------- ------ -------- --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pump_curves = [\"Pump_Curve_\"+curve for curve in pump_curves]\n",
    "pump_status = [\"ON\" for pump in pump_ids]\n",
    "pump_startup = [0 for pump in pump_ids]\n",
    "pump_shutoff = [0 for pump in pump_ids]\n",
    "\n",
    "pump_section=pd.DataFrame(zip(pump_ids,pump_from,pump_to,pump_curves,pump_status,pump_startup,pump_shutoff))\n",
    "\n",
    "pump_section = pump_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "pump_section = [line+'\\n' for line in pump_section]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing X-Sections\n",
    "Link    Shape     Geom1 (DIAMETER)    Geom2 (HW Coefficient)     Geom3    Geom 4       Barrels     Culvert (EMPTY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=[\"FORCE_MAIN\"]*len(conduits.index)\n",
    "hwcoeffs=[130]*len(shape)\n",
    "geom3=[0]*len(shape)\n",
    "geom4=geom3\n",
    "nbarrels=[1]*len(shape)\n",
    "\n",
    "xsections_section=pd.DataFrame(zip(conduit_ids,shape,conduits[\"diameter\"],hwcoeffs,geom3,geom4,nbarrels))\n",
    "xsections_section=xsections_section.to_string(header=False,index=False, col_space=10).splitlines()\n",
    "xsections_section=[line+'\\n' for line in xsections_section]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Curves\n",
    "Storage Curves  \n",
    "Outlet Curves  \n",
    "Pump Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves_name=[]\n",
    "curves_type=[]\n",
    "curves_x=[]\n",
    "curves_y=[]\n",
    "\n",
    "\n",
    "constant_demands=[demand*float(supply_hh)/24 for demand in desired_demands]\n",
    "for i,j in zip(demand_nodes,constant_demands):\n",
    "    curves_name+=[\"Demand\"+str(i),\"Demand\"+str(i),';']\n",
    "    curves_type+=[\"Rating\",\" \",\"  \"]\n",
    "    curves_x+=[0,0.01,\" \"]\n",
    "    curves_y+=[0,j*1000,\" \"]\n",
    "\n",
    "# Reservoir Storage Curves\n",
    "# reservoir_volume=sum(storage_areas)\n",
    "reservoir_volume = sum(storage_areas) * (n_days - 1 + 2)\n",
    "for curve,head,elevation in zip(reservoir_curves,reservoir_heads.values(),reservoir_elevations):\n",
    "    j=float(head)-float(elevation)\n",
    "    for depth in [0,j-2,j-1,j]:\n",
    "        curves_name.append(curve)\n",
    "        curves_x.append(depth)\n",
    "        if depth<=j-2:\n",
    "            curves_y.append(1)\n",
    "        else: curves_y.append(reservoir_volume)\n",
    "        if depth==0:\n",
    "            curves_type.append(\"Storage\")\n",
    "        else: \n",
    "            curves_type.append(\" \")\n",
    "    curves_name.append(\";\")\n",
    "    curves_type.append(\" \")\n",
    "    curves_x.append(\" \")\n",
    "    curves_y.append(\" \")\n",
    "\n",
    "for curve in pcurves:\n",
    "    if len(pcurves[curve]) == 1:\n",
    "        design_flow = pcurves[curve][0][0] * 1000\n",
    "        design_head = pcurves[curve][0][1]\n",
    "\n",
    "        shutoff_head = 1.33 * design_head \n",
    "        max_flow = 2 * design_flow\n",
    "    # Fit an H(Q) = H_max - B * Q^2 Function\n",
    "        B = (shutoff_head - design_head) / design_flow ** 2\n",
    "        flows = []\n",
    "        heads = []\n",
    "        for flow in np.arange(0, max_flow + max_flow/20, max_flow/20):\n",
    "            flows.append(flow)\n",
    "            heads.append(shutoff_head - B * flow **2)\n",
    "    for flow, head in zip(flows, heads):\n",
    "        curves_name.append(\"Pump_Curve_\"+ curve)\n",
    "        if flow  == flows[0]:\n",
    "            curves_type.append(\"Pump3\")\n",
    "        else: curves_type.append(\"  \")\n",
    "        curves_x.append(flow)\n",
    "        curves_y.append(head)\n",
    "curves=pd.DataFrame(list(zip(curves_name,curves_type,curves_x,curves_y)))\n",
    "curves_section=curves.to_string(header=False,index=False,col_space=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Rules Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1\n"
     ]
    }
   ],
   "source": [
    "# Step for control curve generation. The smaller the step, the smoother the curve\n",
    "step=0.002\n",
    "# Parameters for Mauro de Marchis et al. (2015)'s float valve emitter law\n",
    "m, n= 2.5, 4\n",
    "h_min, h_max = 0.9, tank_height\n",
    "\n",
    "# Intializing string for control curves and rules\n",
    "control_curves=\"\"\n",
    "control_rules=\"\"\n",
    "\n",
    "# Diurnal demand pattern defined as a list of multipliers for the base demand\n",
    "pattern=[0.8,0.7,0.6,0.5,0.5,0.5,0.6,0.8,1.2,1.3,1.2,1.2,1.2,1.2,1.2,1.2,1.1,1.1,1.1,1.2,1.3,1.3,1.1,1] \n",
    "times=np.arange(0,int(supply_hh)*50,1)\n",
    "timesrs_pat=\"\"\n",
    "for time in np.arange(0,len(times)):\n",
    "    if time >=len(pattern):\n",
    "        days=math.floor(time/len(pattern))\n",
    "        time_24=time-days*len(pattern)\n",
    "    else: time_24=time\n",
    "    timesrs_pat+=\"Pattern\\t\"+str(time)+\"\\t\"+str(pattern[time_24])+\"\\n\"\n",
    "\n",
    "for outlet in outletdemand.iloc[:,0]:\n",
    "    out_name=\"Outlet\"+outlet[12:]\n",
    "\n",
    "    storage_name=outletdemand.loc[outletdemand[0]==outlet,1].iloc[0]\n",
    "    control_curves+=\"Control\"+out_name+\"\\tControl\"\n",
    "    for height in np.arange(h_min,h_max+step,step):\n",
    "        red_coeff=np.tanh(m*(h_max-height)/(h_max-h_min))*np.tanh(n*(h_max-height)/(h_max-h_min))\n",
    "        if height==h_min:\n",
    "            control_curves+=\"\\t\\t\"+str(round(height,6))+\"\\t\"+str(round(red_coeff,6))+'\\n'\n",
    "        else:\n",
    "            control_curves+=\"Control\"+out_name+\"\\t\\t\"+\"\\t\"+str(round(height,6))+\"\\t\"+str(round(red_coeff,6))+'\\n'\n",
    "    control_curves+=\";\\n\"\n",
    "\n",
    "    control_rules+=\"Rule \"+out_name+\"\\n\"\n",
    "    control_rules+=\"IF NODE \"+storage_name+\" DEPTH >= \"+str(h_min)+\"\\n\"\n",
    "    control_rules+=\"THEN OUTLET \"+out_name+\" SETTING = CURVE \"+\"Control\"+out_name+\"\\n\\n\"\n",
    "\n",
    "# Add rule to stop supply by iterating over reservoir adjacent pipes\n",
    "control_rules+=\"Rule STOPSUPPLY\\n\"\n",
    "control_rules+=\"IF SIMULATION CLOCKTIME > \"+str(supply_hh)+\":\"+str(supply_mm)+\"\\n\"\n",
    "reservoirs_list = network.reservoir_name_list\n",
    "for reservoir in reservoirs_list:\n",
    "    pipes = network.get_links_for_node(reservoir)\n",
    "    for pipe in pipes:\n",
    "        if network.get_link(pipe).length > maximum_xdelta:\n",
    "            suffix = '-1'\n",
    "        else: suffix = ''\n",
    "        if reservoir == reservoirs_list[0]:\n",
    "            print(pipes[0])\n",
    "            control_rules+=\"THEN CONDUIT \"+pipe+suffix+\" STATUS = CLOSED\\n\"\n",
    "        else: control_rules+=\"AND CONDUIT \"+pipe+suffix+\" STATUS = CLOSED\\n\"\n",
    "control_rules+=\"\\n\"\n",
    "\n",
    "# Add rule to start supply by iterating over reservoir adjacent pipes\n",
    "control_rules+=\"Rule STARTSUPPLY\\n\"\n",
    "control_rules+=\"IF SIMULATION CLOCKTIME >= 0:00\\n\"\n",
    "control_rules += \"AND SIMULATION CLOCKTIME < \"+str(supply_hh)+\":\"+str(supply_mm)+\"\\n\"\n",
    "for reservoir in reservoirs_list:\n",
    "    pipes = network.get_links_for_node(reservoir)\n",
    "    for pipe in pipes:\n",
    "        if network.get_link(pipe).length > maximum_xdelta:\n",
    "            suffix = '-1'\n",
    "        else: suffix = ''\n",
    "        if reservoir == reservoirs_list[0]:\n",
    "            control_rules+=\"THEN CONDUIT \"+pipe+suffix+\" STATUS = OPEN\\n\"\n",
    "        else: control_rules+=\"AND CONDUIT \"+pipe+suffix+\" STATUS = OPEN\\n\"\n",
    "control_rules+=\"\\n\"\n",
    "\n",
    "\n",
    "control_rules+=\"Rule Patterns\\n\"\n",
    "control_rules+=\"IF SIMULATION TIME > 0\\n\"\n",
    "flag=0\n",
    "for outlet in outletdemand.iloc[:,0]:\n",
    "    if flag==0:\n",
    "        control_rules+=\"THEN OUTLET \"+outlet+\" SETTING = TIMESERIES Pattern\\n\"\n",
    "        flag=1\n",
    "    else: control_rules+=\"AND OUTLET \"+outlet+\" SETTING = TIMESERIES Pattern\\n\"\n",
    "\n",
    "curves_section+=\"\\n\"+control_curves\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_demand= { node: coords[node] for node in demand_nodes}\n",
    "coords_ids=list(junctions.index)+reservoir_ids_new+storage_ids+outfall_ids+leak_outfall_id\n",
    "offset=2\n",
    "\n",
    "coords_x1=[coord[0] for coord in junctions[\"Coordinates\"]]\n",
    "coords_x2=[coord[0] for coord in reservoir_coords.values()]\n",
    "coords_x3=[coord[0] +2*offset for coord in coords_demand.values()]\n",
    "coords_x4=[coord[0] + 3 * offset for coord in coords_demand.values()]\n",
    "coords_x5=[coord[0] + offset for coord in coords_demand.values()]\n",
    "coords_x=coords_x1+coords_x2+coords_x3+coords_x4+coords_x5\n",
    "\n",
    "coords_y1=[coord[1] for coord in junctions[\"Coordinates\"]]\n",
    "coords_y2=[coord[1] for coord in reservoir_coords.values()]\n",
    "coords_y3=[coord[1] for coord in coords_demand.values()]\n",
    "coords_y4=[coord[1]+ offset for coord in coords_demand.values()]\n",
    "coords_y5=[coord[1] - 1.5 * offset for coord in coords_demand.values()]\n",
    "coords_y=coords_y1+coords_y2+coords_y3+coords_y4+coords_y5\n",
    "\n",
    "coordinate_section=pd.DataFrame(zip(coords_ids,coords_x,coords_y))\n",
    "coordinate_section=coordinate_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "coordinate_section=[line+'\\n' for line in coordinate_section]\n",
    "\n",
    "#Setting View Dimensions\n",
    "x_left=min(coords_x)\n",
    "x_right=max(coords_x)\n",
    "y_down=min(coords_y)\n",
    "y_up=max(coords_y)\n",
    "dimensions_line=str(x_left)+\" \"+str(y_down)+\" \"+str(x_right)+\" \"+str(y_up)+\"\\n\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the SWMM .inp File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opens .inp file to read\n",
    "template=pathlib.Path(\"../Networks/Empty_SWMM_Template.inp\")\n",
    "file=open(template,'r')\n",
    "lines=[]              # list to store all lines in the .inp file\n",
    "linecount=0           # Counter for the number of lines\n",
    "\n",
    "# Loops over each line in the input file \n",
    "for line in file:\n",
    "    if re.search(\"^START_DATE\",line):\n",
    "        start_date = line[11:].lstrip().split('/')\n",
    "    if re.search(\"^END_TIME\",line):\n",
    "        end_time=linecount\n",
    "    if re.search(\"^ROUTING_STEP\",line):\n",
    "        routing_step=linecount\n",
    "    if re.search(\"^END_DATE\", line):\n",
    "        end_date = linecount\n",
    "    if re.search(\"^DIMENSIONS\",line):\n",
    "        dimensions=linecount\n",
    "    # Record the position of the phrase [JUNCTIONS] and add 3 to skip the header lines\n",
    "    if re.search('\\[JUNCTIONS\\]',line):\n",
    "        junctions_marker=linecount+3\n",
    "    # Record the position of the phrase [OUTFALLS] and add 3 to skip the header lines\n",
    "    if re.search('\\[OUTFALLS\\]',line):\n",
    "        outfalls_marker=linecount+3\n",
    "    # Record the position of the phrase [STORAGE] and add 3 to skip the header lines\n",
    "    if re.search('\\[STORAGE\\]',line):\n",
    "        storage_marker=linecount+3\n",
    "    # Record the position of the phrase [CONDUITS] and add 3 to skip the header lines\n",
    "    if re.search('\\[CONDUITS\\]',line):\n",
    "        conduits_marker=linecount+3\n",
    "     # Record the position of the phrase [OUTLETS] and add 3 to skip the header lines\n",
    "    if re.search('\\[PUMPS\\]',line):\n",
    "        pumps_marker=linecount+3\n",
    "    if re.search('\\[OUTLETS\\]',line):\n",
    "        outlets_marker=linecount+3\n",
    "     # Record the position of the phrase [XSECTIONS] and add 3 to skip the header lines\n",
    "    if re.search('\\[XSECTIONS\\]',line):\n",
    "        xsections_marker=linecount+3\n",
    "    if re.search('\\[CONTROLS\\]',line):\n",
    "        controls_marker=linecount+1\n",
    "    # Record the position of the phrase [CURVES] and add 3 to skip the header lines\n",
    "    if re.search('\\[CURVES\\]',line):\n",
    "        curves_marker=linecount+3\n",
    "    if re.search('\\[TIMESERIES\\]',line):\n",
    "        timesrs_marker=linecount+3\n",
    "    # Record the position of the phrase [COORDINATES] and add 3 to skip the header lines\n",
    "    if re.search('\\[COORDINATES\\]',line):\n",
    "        coords_marker=linecount+3\n",
    "    # Store all lines in a list\n",
    "    lines.append(line)\n",
    "    linecount+=1\n",
    "file.close()\n",
    "\n",
    "try:\n",
    "    controls_marker\n",
    "except NameError:\n",
    "    controls_marker=curves_marker-4\n",
    "    control_rules=\"[CONTROLS]\\n\"+control_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file=name_only+'_SWMMIN_'+str(maximum_xdelta)+'m.inp'\n",
    "file=open(directory/output_file,'w')\n",
    "lines[end_time]=\"END_TIME             \"+str(23)+\":\"+str(59)+\":00\\n\"\n",
    "lines[end_date]=\"END_DATE             \"+start_date[0]+\"/\"+str(min((int(start_date[1])+n_days-1),30))+\"/\"+start_date[2]\n",
    "if not timestep:\n",
    "    timestep = maximum_xdelta / 100\n",
    "lines[routing_step]=\"ROUTING_STEP         \"+str(timestep)+\"\\n\"\n",
    "lines[dimensions]=\"DIMENSIONS \"+dimensions_line\n",
    "lines[coords_marker:coords_marker]=coordinate_section\n",
    "lines[timesrs_marker:timesrs_marker]=timesrs_pat\n",
    "lines[curves_marker:curves_marker]=curves_section\n",
    "lines[controls_marker:controls_marker]=control_rules\n",
    "lines[xsections_marker:xsections_marker]=xsections_section\n",
    "lines[outlets_marker:outlets_marker]=outlet_section\n",
    "if len(pump_section)>3:\n",
    "    lines[pumps_marker:pumps_marker]=pump_section\n",
    "lines[conduits_marker:conduits_marker]=conduits_section\n",
    "lines[storage_marker:storage_marker]=storage_section\n",
    "lines[outfalls_marker:outfalls_marker]=outfall_section\n",
    "lines[junctions_marker:junctions_marker]=junctions_section\n",
    "\n",
    "\n",
    "# All lines added by this script are missing a new line character at the end, the conditional statements below add the new line character for these lines only and writes all lines to the file\n",
    "for line in lines:\n",
    "    file.write(line)    \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWMMIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
